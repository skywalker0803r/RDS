{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from pytorch_lightning import seed_everything\n",
    "seed = 12\n",
    "def set_seed(seed = int):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    seed_everything(seed)\n",
    "    return random_state\n",
    "random_state = set_seed(seed)\n",
    "\n",
    "url = 'https://gist.githubusercontent.com/skywalker0803r/e0ef295d69aba240e6ff1537be354bc8/raw/eab83e022de41cfedf1644bd4e5752c97b2488f8/Modeling.py'\n",
    "exec(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "data = joblib.load('data.pkl')\n",
    "merged_df = data['merged_df']\n",
    "input_col = data['input_col']\n",
    "output_col = data['output_col']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ProductDSARRDS2',\n",
       " 'ProductDSARRDS2.1',\n",
       " 'ProductDSARRDS2.10',\n",
       " 'ProductDSARRDS2.11',\n",
       " 'ProductDSARRDS2.12',\n",
       " 'ProductDSARRDS2.13',\n",
       " 'ProductDSARRDS2.14',\n",
       " 'ProductDSARRDS2.15',\n",
       " 'ProductDSARRDS2.16',\n",
       " 'ProductDSARRDS2.17']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(merged_df.columns.tolist())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://gist.githubusercontent.com/skywalker0803r/c92f4651ccbd0640c2a8c97c017092d9/raw/adedbf1159e92402f32b8d961f6db70a1b0c95ad/DataPreprocessingArmory.py'\n",
    "exec(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 47, 113, 47)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_col = set(merged_df.columns)&set(input_col)\n",
    "y_col = set(merged_df.columns)&set(output_col)\n",
    "len(x_col),len(y_col),len(input_col),len(output_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_col.issubset(input_col) ,y_col.issubset(output_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,val_loader = df_2_torch_data_iter(df=merged_df,x_col=x_col,y_col=y_col,test_size=0.2,random_state=seed,batch_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=113, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=128, out_features=47, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_mlp(\n",
    "    layers = [len(x_col),128,128,len(y_col)],\n",
    "    activation_function = nn.ReLU(),\n",
    "    output_activation_function = nn.Sigmoid(),\n",
    "    )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:00<00:00, 131.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train_loss:0.04177649598568678 val_loss:0.03124733828008175\n",
      "save best_model now_val_best_loss is:0.03124733828008175\n",
      "epoch:1 train_loss:0.03900280874222517 val_loss:0.028774630278348923\n",
      "save best_model now_val_best_loss is:0.028774630278348923\n",
      "epoch:2 train_loss:0.036057026125490665 val_loss:0.026228483766317368\n",
      "save best_model now_val_best_loss is:0.026228483766317368\n",
      "epoch:3 train_loss:0.033633201848715544 val_loss:0.024401331320405006\n",
      "save best_model now_val_best_loss is:0.024401331320405006\n",
      "epoch:4 train_loss:0.032288978807628155 val_loss:0.023246927186846733\n",
      "save best_model now_val_best_loss is:0.023246927186846733\n",
      "epoch:5 train_loss:0.03140040813013911 val_loss:0.022412609308958054\n",
      "save best_model now_val_best_loss is:0.022412609308958054\n",
      "epoch:6 train_loss:0.03064885176718235 val_loss:0.021931329742074013\n",
      "save best_model now_val_best_loss is:0.021931329742074013\n",
      "epoch:7 train_loss:0.030022671911865473 val_loss:0.021547826007008553\n",
      "save best_model now_val_best_loss is:0.021547826007008553\n",
      "epoch:8 train_loss:0.029373178724199533 val_loss:0.021050501614809036\n",
      "save best_model now_val_best_loss is:0.021050501614809036\n",
      "epoch:9 train_loss:0.02853342005982995 val_loss:0.020340561866760254\n",
      "save best_model now_val_best_loss is:0.020340561866760254\n",
      "epoch:10 train_loss:0.02747369511052966 val_loss:0.0195340383797884\n",
      "save best_model now_val_best_loss is:0.0195340383797884\n",
      "epoch:11 train_loss:0.026167490053921938 val_loss:0.018632184714078903\n",
      "save best_model now_val_best_loss is:0.018632184714078903\n",
      "epoch:12 train_loss:0.024651456624269485 val_loss:0.017609605565667152\n",
      "save best_model now_val_best_loss is:0.017609605565667152\n",
      "epoch:13 train_loss:0.022947075311094522 val_loss:0.016658786684274673\n",
      "save best_model now_val_best_loss is:0.016658786684274673\n",
      "epoch:14 train_loss:0.021235016640275717 val_loss:0.015771914273500443\n",
      "save best_model now_val_best_loss is:0.015771914273500443\n",
      "epoch:15 train_loss:0.01971223414875567 val_loss:0.015344788320362568\n",
      "save best_model now_val_best_loss is:0.015344788320362568\n",
      "epoch:16 train_loss:0.018542062724009156 val_loss:0.015280207619071007\n",
      "save best_model now_val_best_loss is:0.015280207619071007\n",
      "epoch:17 train_loss:0.017750000581145287 val_loss:0.015404565259814262\n",
      "epoch:18 train_loss:0.017205774784088135 val_loss:0.015585568733513355\n",
      "epoch:19 train_loss:0.016817721305415034 val_loss:0.015709932893514633\n",
      "epoch:20 train_loss:0.016484090127050877 val_loss:0.015772899612784386\n",
      "epoch:21 train_loss:0.016159775899723172 val_loss:0.015757085755467415\n",
      "epoch:22 train_loss:0.015874937875196338 val_loss:0.015681909397244453\n",
      "epoch:23 train_loss:0.015597086632624269 val_loss:0.015588048845529556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:00<00:00, 133.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 train_loss:0.015328767942264676 val_loss:0.015544583089649677\n",
      "epoch:25 train_loss:0.015092830872163177 val_loss:0.015527606010437012\n",
      "epoch:26 train_loss:0.014846455305814743 val_loss:0.015507210977375507\n",
      "epoch:27 train_loss:0.014602505369111896 val_loss:0.015464193187654018\n",
      "epoch:28 train_loss:0.014387988019734621 val_loss:0.015409269370138645\n",
      "epoch:29 train_loss:0.014173268340528011 val_loss:0.015390250831842422\n",
      "epoch:30 train_loss:0.013989725848659873 val_loss:0.015387295745313168\n",
      "epoch:31 train_loss:0.013813969446346164 val_loss:0.015398218296468258\n",
      "epoch:32 train_loss:0.013661238830536604 val_loss:0.015395531430840492\n",
      "epoch:33 train_loss:0.013523454079404473 val_loss:0.015387258492410183\n",
      "epoch:34 train_loss:0.01339422701857984 val_loss:0.015412175096571445\n",
      "epoch:35 train_loss:0.013287434354424477 val_loss:0.015430476516485214\n",
      "epoch:36 train_loss:0.013181583723053336 val_loss:0.015430203638970852\n",
      "epoch:37 train_loss:0.013087255414575338 val_loss:0.01542653702199459\n",
      "epoch:38 train_loss:0.012991457246243954 val_loss:0.015426281839609146\n",
      "epoch:39 train_loss:0.012903035385534167 val_loss:0.015436285175383091\n",
      "epoch:40 train_loss:0.012813552282750607 val_loss:0.015442634001374245\n",
      "epoch:41 train_loss:0.012728711124509573 val_loss:0.015428387559950352\n",
      "epoch:42 train_loss:0.012636441038921475 val_loss:0.015422394499182701\n",
      "epoch:43 train_loss:0.012555713299661875 val_loss:0.015415556728839874\n",
      "epoch:44 train_loss:0.012447603978216648 val_loss:0.015434595756232738\n",
      "epoch:45 train_loss:0.012383463559672236 val_loss:0.015419977717101574\n",
      "epoch:46 train_loss:0.012275319546461105 val_loss:0.015388088300824165\n",
      "epoch:47 train_loss:0.012194522423669696 val_loss:0.015346785075962543\n",
      "epoch:48 train_loss:0.012098988518118858 val_loss:0.01532437838613987\n",
      "epoch:49 train_loss:0.011996683664619923 val_loss:0.015312210656702518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:00<00:00, 130.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:50 train_loss:0.01190401311032474 val_loss:0.015290599316358566\n",
      "epoch:51 train_loss:0.011791423545219004 val_loss:0.015269850380718708\n",
      "save best_model now_val_best_loss is:0.015269850380718708\n",
      "epoch:52 train_loss:0.011696703149937093 val_loss:0.015279951505362988\n",
      "epoch:53 train_loss:0.011589032365009189 val_loss:0.015269409865140915\n",
      "save best_model now_val_best_loss is:0.015269409865140915\n",
      "epoch:54 train_loss:0.011482084169983864 val_loss:0.015206647105515003\n",
      "save best_model now_val_best_loss is:0.015206647105515003\n",
      "epoch:55 train_loss:0.01136532612144947 val_loss:0.015201972797513008\n",
      "save best_model now_val_best_loss is:0.015201972797513008\n",
      "epoch:56 train_loss:0.01126302604097873 val_loss:0.01521767396479845\n",
      "epoch:57 train_loss:0.011149359866976738 val_loss:0.01519713643938303\n",
      "save best_model now_val_best_loss is:0.01519713643938303\n",
      "epoch:58 train_loss:0.011031693546101451 val_loss:0.015160002745687962\n",
      "save best_model now_val_best_loss is:0.015160002745687962\n",
      "epoch:59 train_loss:0.010917229810729623 val_loss:0.015114491805434227\n",
      "save best_model now_val_best_loss is:0.015114491805434227\n",
      "epoch:60 train_loss:0.010812659398652613 val_loss:0.015087181702256203\n",
      "save best_model now_val_best_loss is:0.015087181702256203\n",
      "epoch:61 train_loss:0.010677438112907112 val_loss:0.015112753957509995\n",
      "epoch:62 train_loss:0.010588040458969772 val_loss:0.015176868997514248\n",
      "epoch:63 train_loss:0.010441429214552045 val_loss:0.01507011242210865\n",
      "save best_model now_val_best_loss is:0.01507011242210865\n",
      "epoch:64 train_loss:0.010352501296438277 val_loss:0.01507733017206192\n",
      "epoch:65 train_loss:0.010206383070908487 val_loss:0.015113512054085732\n",
      "epoch:66 train_loss:0.010106334928423166 val_loss:0.015074554830789566\n",
      "epoch:67 train_loss:0.009983196505345404 val_loss:0.015057578682899475\n",
      "save best_model now_val_best_loss is:0.015057578682899475\n",
      "epoch:68 train_loss:0.009867476299405098 val_loss:0.015115801244974136\n",
      "epoch:69 train_loss:0.009758685366250575 val_loss:0.01510204840451479\n",
      "epoch:70 train_loss:0.009630049928091466 val_loss:0.015047604218125343\n",
      "save best_model now_val_best_loss is:0.015047604218125343\n",
      "epoch:71 train_loss:0.009533363860100508 val_loss:0.01507400069385767\n",
      "epoch:72 train_loss:0.009408955927938223 val_loss:0.015059946104884148\n",
      "epoch:73 train_loss:0.009299609810113907 val_loss:0.015035784803330898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 134.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save best_model now_val_best_loss is:0.015035784803330898\n",
      "epoch:74 train_loss:0.009200337692163885 val_loss:0.015089861117303371\n",
      "epoch:75 train_loss:0.009086708421818912 val_loss:0.015101412311196327\n",
      "epoch:76 train_loss:0.008979764650575817 val_loss:0.015018955804407597\n",
      "save best_model now_val_best_loss is:0.015018955804407597\n",
      "epoch:77 train_loss:0.008885472198016942 val_loss:0.015069322660565376\n",
      "epoch:78 train_loss:0.008781522861681879 val_loss:0.015064364299178123\n",
      "epoch:79 train_loss:0.008681389619596303 val_loss:0.015056224539875984\n",
      "epoch:80 train_loss:0.008587488089688122 val_loss:0.015109997242689133\n",
      "epoch:81 train_loss:0.008489254978485405 val_loss:0.015010381117463112\n",
      "save best_model now_val_best_loss is:0.015010381117463112\n",
      "epoch:82 train_loss:0.008403258863836527 val_loss:0.015061295591294765\n",
      "epoch:83 train_loss:0.008318664971739054 val_loss:0.01507723331451416\n",
      "epoch:84 train_loss:0.008220867835916579 val_loss:0.0151071697473526\n",
      "epoch:85 train_loss:0.008145150728523731 val_loss:0.01515275426208973\n",
      "epoch:86 train_loss:0.008072981727309525 val_loss:0.01506223063915968\n",
      "epoch:87 train_loss:0.007983783492818475 val_loss:0.01508442871272564\n",
      "epoch:88 train_loss:0.007908983738161623 val_loss:0.015137366950511932\n",
      "epoch:89 train_loss:0.007826109183952212 val_loss:0.015027676708996296\n",
      "epoch:90 train_loss:0.0077534004813060164 val_loss:0.015172961167991161\n",
      "epoch:91 train_loss:0.0076892595971003175 val_loss:0.015147767029702663\n",
      "epoch:92 train_loss:0.007615568465553224 val_loss:0.01510592270642519\n",
      "epoch:93 train_loss:0.007540137507021427 val_loss:0.015082220546901226\n",
      "epoch:94 train_loss:0.007484804140403867 val_loss:0.01513823214918375\n",
      "epoch:95 train_loss:0.007415923522785306 val_loss:0.015054337680339813\n",
      "epoch:96 train_loss:0.007352792541496456 val_loss:0.015200424008071423\n",
      "epoch:97 train_loss:0.007292212685570121 val_loss:0.015108812600374222\n",
      "epoch:98 train_loss:0.007237019133754075 val_loss:0.015087002888321877\n",
      "epoch:99 train_loss:0.00717976049054414 val_loss:0.015185018070042133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "def step(model,batch,optimizer,device,loss_fn,mode='train'):\n",
    "    '''\n",
    "    這個函數實現了模型前向&後向傳播的過程,稱之為step\n",
    "    '''\n",
    "    # model to device\n",
    "    model = model.to(device)\n",
    "    # change mode(train/val)\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    if mode == 'val':\n",
    "        model.eval()\n",
    "    # model forward pass\n",
    "    x,y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    output = model(x)\n",
    "    loss = loss_fn(y,output)\n",
    "    # update model if mode == train\n",
    "    if mode == 'train': \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def train_loop(model,train_loader,val_loader,optimizer,device,loss_fn,max_epochs=12):\n",
    "    history = {'train_loss':[],'val_loss':[]}\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    for epoch in tqdm(range(max_epochs)):        \n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        for i,batch in enumerate(train_loader):\n",
    "            epoch_train_loss += step(model,batch,optimizer,device,loss_fn,mode='train')\n",
    "        for j,batch in enumerate(val_loader):\n",
    "            epoch_val_loss += step(model,batch,optimizer,device,loss_fn,mode='val')\n",
    "        history['train_loss'].append(epoch_train_loss/(i+1))\n",
    "        history['val_loss'].append(epoch_val_loss/(j+1))\n",
    "        print(f'epoch:{epoch} train_loss:{epoch_train_loss/(i+1)} val_loss:{epoch_val_loss/(j+1)}')\n",
    "        if history['val_loss'][-1] <= best_loss: \n",
    "            best_model = deepcopy(model.eval())\n",
    "            best_loss = history['val_loss'][-1]\n",
    "            print(f'save best_model now_val_best_loss is:{best_loss}')\n",
    "    return best_model.eval() ,history\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params = model.parameters(),\n",
    "    lr = 1e-3)\n",
    "device = 'cpu'\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "best_model ,history = train_loop(model,train_loader,val_loader,optimizer,device,loss_fn,max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyPUlEQVR4nO3deZgdVZ34//fn3r697/ue7iSdrTtkoROCILKTBCQ4Ct+gLMPMgDyKg4w6hBkd8TeOMn7VEX4CCgqiIsgAQmRHJEYkgXQWks5GOp2t9yW978v5/lHVyU2nk7693L7dXZ/X89zn3qo6VXVOL/W559Spc8QYg1JKKedxBToDSimlAkMDgFJKOZQGAKWUcigNAEop5VAaAJRSyqGCAp2BkUhMTDQ5OTmBzoZSSk0pW7durTPGJA1eP6UCQE5ODkVFRYHOhlJKTSkicmSo9doEpJRSDqUBQCmlHEoDgFJKOdSUugeglFIj1dPTQ1lZGZ2dnYHOit+FhoaSmZmJx+PxKb0GAKXUtFZWVkZUVBQ5OTmISKCz4zfGGOrr6ykrKyM3N9enfbQJSCk1rXV2dpKQkDCtL/4AIkJCQsKIajoaAJRS0950v/gPGGk5HREA3t1XwyMbSgKdDaWUmlQcEQDeP1jHT/50gJ6+/kBnRSnlQI2NjTzyyCMj3m/16tU0NjaOf4ZsjggA+ekxdPf2c7C2NdBZUUo50JkCQF9f31n3e+2114iNjfVTrhwSAAoyogHYXd4c4JwopZxo3bp1HDx4kMWLF7Ns2TIuueQSPv/5z7Nw4UIArrvuOs4991zy8/N57LHHTuyXk5NDXV0dhw8fZv78+dx+++3k5+dz5ZVX0tHRMeZ8OaIbaG5iJGEeN8UVTXz23MxAZ0cpFSDf+eNu9lSM7xfBBenRfPvT+WdN88ADD1BcXMyOHTvYsGEDV199NcXFxSe6az7xxBPEx8fT0dHBsmXL+OxnP0tCQsIpxzhw4ADPPPMMjz/+ODfccAMvvPACN91005jy7ogA4HYJ89OitAaglJoUli9ffkpf/Yceeog//OEPABw7dowDBw6cFgByc3NZvHgxAOeeey6HDx8ecz4cEQAACjJieHFbOf39BpfLGV3ClFKnGu6b+kSJiIg48XnDhg386U9/YtOmTYSHh3PxxRcP2Zc/JCTkxGe32z0uTUCOuAcAkJ8eTWtXL0eOtwc6K0oph4mKiqKlpWXIbU1NTcTFxREeHs6+ffvYvHnzhOXLpwAgIitFZL+IlIjIuiG2i4g8ZG/fKSJLB213i8h2EXnFa128iLwtIgfs97ixF+fM8tNjACgub/LnaZRS6jQJCQlccMEFFBQU8I1vfOOUbStXrqS3t5dzzjmHb33rW6xYsWLC8jVsE5CIuIGHgSuAMmCLiKw3xuzxSrYKyLNf5wGP2u8D7gb2AtFe69YB7xhjHrCDyjrg3jGU5azmpEThcQu7K5r59KJ0f51GKaWG9Lvf/W7I9SEhIbz++utDbhto509MTKS4uPjE+q9//evjkidfagDLgRJjTKkxpht4FlgzKM0a4NfGshmIFZE0ABHJBK4GfjHEPk/Zn58CrhtdEXwTHORiTkoUuyu0BqCUUuBbAMgAjnktl9nrfE3zE+BfgcGP4aYYYyoB7PfkoU4uIneISJGIFNXW1vqQ3TMrSI+huLwJY8yYjqOUUtOBLwFgqC4zg6+gQ6YRkWuAGmPM1hHnbOAgxjxmjCk0xhQmJZ02p/GIFGRE09DeQ2XT9B8XXCmlhuNLACgDsryWM4EKH9NcAFwrIoexmo4uFZHf2mmqvZqJ0oCaEed+hBbojWCllDrBlwCwBcgTkVwRCQbWAusHpVkP3GL3BloBNBljKo0x9xljMo0xOfZ+fzbG3OS1z63251uBl8damOHMT4vCJVA8zk8CKqXUVDRsLyBjTK+I3AW8CbiBJ4wxu0XkTnv7z4DXgNVACdAO3ObDuR8AnhORfwSOAtePrgi+Cw8OYmZSJHv0RrBSSvn2JLAx5jWsi7z3up95fTbAl4c5xgZgg9dyPXCZ71kdHwXp0WwqrZ/o0yqllM8iIyNpbfX/6MWOeRJ4QH56DNXNXdS3dgU6K0opFVCOGQtowPw061m0fVUtXDA7ZJjUSik1dvfeey8zZszgS1/6EgD3338/IsLGjRtpaGigp6eH7373u6xZM/gRK/9yXACYlxYFwN7KZi6YnRjg3CilJtTr66Bq1/geM3UhrHrgrEnWrl3LV7/61RMB4LnnnuONN97gnnvuITo6mrq6OlasWMG11147ofMXOy4AJEaGkBQVwp5K7QmklJoYS5YsoaamhoqKCmpra4mLiyMtLY177rmHjRs34nK5KC8vp7q6mtTU1AnLl+MCAFjNQPsqhx6ZTyk1jQ3zTd2fPve5z/H8889TVVXF2rVrefrpp6mtrWXr1q14PB5ycnKGHAbanxx3Exis5wFKalp1knil1IRZu3Ytzz77LM8//zyf+9znaGpqIjk5GY/Hw7vvvsuRI0cmPE/ODACp0XT36STxSqmJk5+fT0tLCxkZGaSlpfGFL3yBoqIiCgsLefrpp5k3b96E58mxTUBg3Qielxo9TGqllBofu3advAGdmJjIpk2bhkw3Ec8AgENrADOTIgh2u/Q+gFLK0RwZADxuF3kpkdoTSCnlaI4MAGA1A+3VGoBSjuCUOUBGWk7HBoB5qVHUtXZR26JDQig1nYWGhlJfXz/tg4Axhvr6ekJDQ33ex5E3gQEWnBgSopmkqLFNNKOUmrwyMzMpKytjrDMKTgWhoaFkZmb6nN6xAcC7J9An8zQAKDVdeTwecnNzA52NScmxTUBxEcGkRofqfQCllGM5NgCA9UTwXu0JpJRyKEcHgHlp0RysbaWrty/QWVFKqQnn6ABwTkYMPX1Gm4GUUo7k6ACwJDsOgG1HGgKcE6WUmniODgCpMaGkx4Sy7agGAKWU8/gUAERkpYjsF5ESEVk3xHYRkYfs7TtFZKm9PlREPhSRj0Rkt4h8x2uf+0WkXER22K/V41cs3y2ZEcf2o42BOLVSSgXUsAFARNzAw8AqYAFwo4gsGJRsFZBnv+4AHrXXdwGXGmMWAYuBlSKywmu//zHGLLZfr42pJKO0NDuO8sYOqpsndiIGpZQKNF9qAMuBEmNMqTGmG3gWGDxz8Rrg18ayGYgVkTR7eWBcU4/9mlTPYy/NjgVguzYDKaUcxpcAkAEc81ous9f5lEZE3CKyA6gB3jbGfOCV7i67yegJEYkb6uQicoeIFIlIkT8e5c5PjyE4yMU2bQZSSjmMLwFgqCnqB3+LP2MaY0yfMWYxkAksF5ECe/ujwCyspqFK4EdDndwY85gxptAYU5iUNP5DNgQHuShIj9aeQEopx/ElAJQBWV7LmUDFSNMYYxqBDcBKe7naDg79wONYTU0BsTQ7jp3lTXT36hzBSinn8CUAbAHyRCRXRIKBtcD6QWnWA7fYvYFWAE3GmEoRSRKRWAARCQMuB/bZy2le+38GKB5bUUZv6Yw4unv7dYIYpZSjDDsaqDGmV0TuAt4E3MATxpjdInKnvf1nwGvAaqAEaAdus3dPA56yexK5gOeMMa/Y234gIouxmooOA18cr0KN1FKvB8IWZ8UGKhtKKTWhfBoO2u6i+dqgdT/z+myALw+x305gyRmOefOIcupHAw+EbT/WGOisKKXUhHH0k8DelsyI0xvBSilH0QBgW5IVS3ljB+WNHYHOilJKTQgNALYrF6QC8MLWsgDnRCmlJoYGAFt2QjifzEvk2Q+P0tc/qR5WVkopv9AA4OXG5dlUNHWy8ePpP3m0UkppAPByxYIUEiNDePqDo4HOilJK+Z0GAC8et4vrCzP5875qqpp0dFCl1PSmAWCQG5dl02/g91uODZ9YKaWmMA0AgwzcDP79Fr0ZrJSa3jQADOEL51k3g1/cpl1ClVLTlwaAIVy5IJWl2bE88Po+mtp7Ap0dpZTyCw0AQ3C5hP+8roCG9m5++Nb+QGdHKaX8QgPAGeSnx3DL+Tn89oMj7CprCnR2lFJq3GkAOIt/uXIOCREhfPPlYvr1hrBSappxRgCoKobdfxjxbtGhHr559Xw+OtbIE3875IeMKaVU4DgjAGz9Fbx8F/T3jXjXNYvTuWJBCj94Yz+7K7QpSCk1fTgjAGQWQncr1I78hq6I8N+fPYfYcA93P7uDju6RBxGllJqMHBIAllnvZVtGtXt8RDA/vmExJTWtfPfVPeOYMaWUChxnBID4mRAWN+oAAHBhXiJ3XDSTpz84yis7K8Yxc0opFRg+BQARWSki+0WkRETWDbFdROQhe/tOEVlqrw8VkQ9F5CMR2S0i3/HaJ15E3haRA/Z73PgV67QMQkYhlBWN6TBfv3IuhTPi+NpzH7HtqE4fqZSa2oYNACLiBh4GVgELgBtFZMGgZKuAPPt1B/Covb4LuNQYswhYDKwUkRX2tnXAO8aYPOAde9l/MpdB7T7obB71IYKDXDx2SyGpMaHc/lQRR+vbxzGDSik1sXypASwHSowxpcaYbuBZYM2gNGuAXxvLZiBWRNLs5VY7jcd+Ga99nrI/PwVcN4ZyDC+z0Dp1xbYxHSY+Ipgn/34Zvf2G2371oQ4VoZSasnwJABmA99jIZfY6n9KIiFtEdgA1wNvGmA/sNCnGmEoA+z15xLkfiYxz7ZyN/j7AgJlJkTx287kcPd7Ol3+3jd6+/jEfUymlJpovAUCGWDf4sdgzpjHG9BljFgOZwHIRKRhJBkXkDhEpEpGi2toxTNUYFguJc6Bs6+iP4eW8mQl87zMLea+kju++undcjqmUUhPJlwBQBmR5LWcCg7vBDJvGGNMIbABW2quqRSQNwH6vGerkxpjHjDGFxpjCpKQkH7J7FpnLrBqAGZ9hHa4vzOL2T+byq/cP8/QHR8blmEopNVF8CQBbgDwRyRWRYGAtsH5QmvXALXZvoBVAkzGmUkSSRCQWQETCgMuBfV773Gp/vhV4eWxF8UHGudBeBw2Hx+2Q61bN5+K5SXz75d18UFo/bsdVSil/GzYAGGN6gbuAN4G9wHPGmN0icqeI3Gknew0oBUqAx4Ev2evTgHdFZCdWIHnbGPOKve0B4AoROQBcYS/718ADYeXj0wwE4HYJD924hMy4ML7+/Ef6pLBSasoQM07NIROhsLDQFBWNoS9/Xy88kAVLb4FV/z1+GQM2l9az9rHN3HHRTP5t9fxxPbZSSo2FiGw1xhQOXu+MJ4EHuIMgfcm49AQabMXMBG5cns0v/lqq8wcopaYEZwUAsJ4HqNwJvV3jfuh1q+aRGBnCvS/spEe7hiqlJjnnBYCMc6G/x5ojYJzFhHn4/9YUsKeymSd1/gCl1CTnwABgN4ON441gbysLUrl4bhKPbDhIS6c+JayUmrycFwCi0yEy1W8BAOBrV8ylsb2HX/3tsN/OoZRSY+W8ACBiNQOVj21k0LNZmBnDFQtSePyvpTR1aC1AKTU5OS8AAGQshfoS6PDfkM5fvTyP5s5ennhP7wUopSYnhwYAe2C4iu1+O0V+egyrClJ54r1DNLZ3++08Sik1Ws4MAOlLrHc/3gcAuPvyPFq6evml1gKUUpOQMwNAWCwk5EH52OYGGM681GiuXJDC0x8cpatXh4hQSk0uzgwAYD0QVlY0biODnsnN58/geFs3r++q8ut5lFJqpJwbADLOhbYaaC7362kumJVIbmIEv92sw0UrpSYXBweApda7n+8DuFzCF87LpuhIA3srRz8fsVJKjTfnBoCUAnAH+z0AAHzu3ExCglxaC1BKTSrODQBBIZC60O83ggFiw4P59KJ0/rC9XIeHUEpNGs4NAGDdB6jYDv3+76Fz04oZtHf38dJ2/95zUEopXzk7AGSvgO7WCakFLMqMoSAjmmc+POb3cymllC+cHQBmXQriho/f8PupRIS/W5LJnspmDlS3+P18Sik1HGcHgLA4qxZw4M0JOd01i9JwCby0Q5uBlFKB5+wAAJB3JVTtgib/X5STo0K5YHYiL++oYCrNxayUmp58CgAislJE9otIiYisG2K7iMhD9vadIrLUXp8lIu+KyF4R2S0id3vtc7+IlIvIDvu1evyKNQJzVlrvB96akNNdtziDsoYOth3130ikSinli2EDgIi4gYeBVcAC4EYRWTAo2Sogz37dATxqr+8FvmaMmQ+sAL48aN//McYstl+vja0oo5Q0F2KzJywAXJmfQkiQi5e2V0zI+ZRS6kx8qQEsB0qMMaXGmG7gWWDNoDRrgF8by2YgVkTSjDGVxphtAMaYFmAvkDGO+R87Eci7Cko3QE+n308XFerh8gUpvLqrUieOV0oFlC8BIAPw7rtYxukX8WHTiEgOsAT4wGv1XXaT0RMiEjfUyUXkDhEpEpGi2tpaH7I7CnNWQk87HH7PP8cf5LrFGRxv6+avB/xUHqWU8oEvAUCGWDf4DuZZ04hIJPAC8FVjzMCAOI8Cs4DFQCXwo6FObox5zBhTaIwpTEpK8iG7o5BzIXjCJ6Q7KMCn5iQRG+7RZiClVED5EgDKgCyv5Uxg8JXrjGlExIN18X/aGPPiQAJjTLUxps8Y0w88jtXUFBieUMj9lNUddAJ65wQHuVhVkMo7e6vp7NF5ApRSgeFLANgC5IlIrogEA2uB9YPSrAdusXsDrQCajDGVIiLAL4G9xpgfe+8gImlei58BikddivEwdyU0Hp2QweEArsxPpa27j/cP1k3I+ZRSarBhA4Axphe4C3gT6ybuc8aY3SJyp4jcaSd7DSgFSrC+zX/JXn8BcDNw6RDdPX8gIrtEZCdwCXDPuJVqNAo+C6Ex8LefTMjpPjErgciQIN4srp6Q8yml1GBBviSyu2i+Nmjdz7w+G+DLQ+z3HkPfH8AYc/OIcupvIVGw7Hb464+g7gAk5vn3dEFuLp6bxJ/2VtPXb3C7hvwxKaWU3+iTwN7Ou9MaJvpvD07I6a7KT6W+rZuiw8cn5HxKKeVNA4C3yCRYchPs/D00V/r9dBfPTSLY7eKtPdoMpJSaeBoABjv/Lujvhc2P+P1UUaEeLpidwJu7q3RsIKXUhNMAMFh8LuR/BoqegHb/N81clZ9KWUMHe3S+YKXUBNMAMJRPfg16OuDt//D7qS6bn4IIvLlbm4GUUhNLA8BQUvLhE3fB9t9A6V/8eqqkqBAKZ8Tx1u4qv55HKaUG0wBwJhffB/Ez4Y//DN3tfj3VVfmp7Ktq4Uh9m1/Po5RS3jQAnIknDD79EDQchg3f8+uprlyQCsDb2htIKTWBNACcTe4nYemtsOlh+Nh/00ZmJ4QzLzWKN7UZSCk1gTQADOfK/4SUAnjmRtj+tP9Ok59K0ZEG6lq7/HYOpZTypgFgOKEx8PevWrWBl78EG/+vX0YMvSo/BWPgT9oMpJSaIBoAfBEaDZ//X1h4A/z5u/D8bdAxvnP6LkiLJiM2TJ8KVkpNGA0AvgoKhs/8HC77Nuz9Izx64bjOICYiXJWfynsH6mjt6h234yql1JloABgJlws++S/wj29Zg8b96hp4//8ft8NfmZ9Cd18/f9mvU0UqpfxPA8BoZJwLX9wIC66Ft74Jf7p/XO4LFM6IIz4iWHsDKaUmhAaA0QqJhM89CefeBu/9D/zxbugf2/SOQW4Xl81L5t19NTpVpFLK7zQAjIXLDdf8jzV20Lan4I11Yz7kNYvSaenqZcP+mnHIoFJKnZkGgLESgcv+wxpG+sPHoPiFMR3uglkJJEYG8/KOinHKoFJKDU0DwHi5/H7IXA7r/xnqSkZ9mCC3i2vOSeedfTU0d/aMX/6UUmoQDQDjxe2B658EdzA8d4s1nPQorVmcTndvP28U681gpZT/+BQARGSliOwXkRIROa2hWywP2dt3ishSe32WiLwrIntFZLeI3O21T7yIvC0iB+z3uPErVoDEZMLfPQ41u+HNfxv1YRZnxTIjIZyXd5SPY+aUUupUwwYAEXEDDwOrgAXAjSKyYFCyVUCe/boDeNRe3wt8zRgzH1gBfNlr33XAO8aYPOAde3nqy7vcuh9Q9AQcfHdUhxAR1ixK5/2D9dQ0d45zBpVSyuJLDWA5UGKMKTXGdAPPAmsGpVkD/NpYNgOxIpJmjKk0xmwDMMa0AHuBDK99nrI/PwVcN7aiTCKXfhMSZlv3A7paRnWIaxdnYAys/0hvBiul/MOXAJABHPNaLuPkRdznNCKSAywBPrBXpRhjKgHs9+ShTi4id4hIkYgU1dZOkSdkPWGw5hFoOgZvfWtUh5idHElBRrQGAKWU3/gSAGSIdYMfez1rGhGJBF4AvmqMGdHs58aYx4wxhcaYwqSkpJHsGljZ58H5X4atT466KegzSzLZWdbErrKmcc6cUkr5FgDKgCyv5Uxg8NfSM6YREQ/Wxf9pY8yLXmmqRSTNTpMGTL8nnwaagl65B3pG3pZ/fWEmkSFB/HzjQT9kTinldL4EgC1AnojkikgwsBZYPyjNeuAWuzfQCqDJGFMpIgL8EthrjPnxEPvcan++FXh51KWYrDxhsPqH0HBoVIPGRYd6+MJ52by2q5Kj9f6dl1gp5TzDBgBjTC9wF/Am1k3c54wxu0XkThG50072GlAKlACPA1+y118A3AxcKiI77Ndqe9sDwBUicgC4wl6efmZdAgvWwF9/BI1HR7z7bRfk4nYJv3yv1A+ZU0o5mRg/zG7lL4WFhaaoqCjQ2Ri5xmPw8HKYfRn8n9+OePdv/O9H/HFnBe+vu4z4iGA/ZFApNZ2JyFZjTOHg9fok8ESIzbIGjNv7Ryh5Z8S733HRTDp7+vnNpiN+yJxSyqk0AEyUT3wF4mfBG/eNeNjovJQoLpuXzFObDtOi4wMppcaJBoCJEhRijRpatx92/n7Eu//zZXk0tnfzX6/u9UPmlFJOpAFgIi1YA2mL4d3vQ2/XiHZdlBXLFz81i2e3HOPdfdOvx6xSauJpAJhIInDZt6DpKGx9avj0g3z18jzmpkRx7ws7aWzv9kMGlVJOogFgos26DGZcCBv/L3S3jWjXkCA3P7phEcfbuvn2+t1+yqBSyik0AEy0gRnE2mpg86PDpx+kICOGr1yax8s7Knhkw+gnnlFKKQ0AgZB9HsxZBX97ENrqRrz7XZfO5tpF6fzgjf38/C86TIRSanQ0AATKFd+xmoDe/d6Id3W7hB/fsIhrzknj+6/v4xd/1aeElVIjpwEgUJLmwrJ/tEYLrRl5184gt4uf/J/FrF6Yyndf3cu3Xiqms2dkzxcopZxNA0AgXXwfhETBm/8+qt2D3C4eXLuEf7owl99sPsKan/6N/VWjm4BGKeU8GgACKTwePnUvHHwHDrw9qkN43C6+ec0CfnXbMurburj2p+/x6IaD9PT1j3NmlVLTjQaAQFt2uzVExJv/Br2j79t/8dxkXr/7Ij41J4n/fmMfVz/0V7YcPj6OGVVKTTcaAAItKBhWfh/qPobNj4zpUElRITx2SyGP31JIW1cf1/9sE9/434+obx3ZU8dKKWfQADAZzLkK5l4Nf/lvaCob8+GuWJDC2/9yEV/81Ez+sL2cS364gd9sPkJf/9QZ+lsp5X8aACaLVQ+AMfDGunE5XHhwEPetms/rd3+SBenRfOulYlY9uJG3dlcxleaAUEr5jwaAySI2Gy76ujVnwChvCA8lLyWKZ25fwcOfX0pPn+GO32zl7x59n/cPjvwBNKXU9KIzgk0mvV3w6CesCeRvegGS5/m+b1MZHN0M5Vuh4Qg0HYOWSmtbUCgEhdAfmUppfwpvVoSxvSOFyOxzuHnlJzk3J8E/5VFKTQpnmhFMA8BkU74NfncD9HTAdY/CgmuHTtffD2Ufwu4/wL5XrQs+gCcc4nIgJhOi0kBc0NcNPe3QXAHHS6Gt9sRhWk0oTcEpxCSkERmfYj2XEBR68uUJtY4ZEgXhCRCeCBGJ1rGDw/3/8/CFMdYkO6bPehcBdzC43IHOmVKTwpkCQJCPO68EHgTcwC+MMQ8M2i729tVAO/D3xpht9rYngGuAGmNMgdc+9wO3AwNXo38zxrw2wnJNPxlL4Ysb4fc3w3M3W91Es1dATBZ4wqC8yPqmf+iv0FIB7hCYfbk141jWeZBSAO5hfq2dzVC7n66KXRzc+QG15YeIqmgkq66cxOBePHQjPZ3Q2wHmLM8ThMRYwcATbgUKd4h18RW7ZbG/F/p6rHewtiHgCgK3x0rX22kNidHdagW9nk5rnfd5B44pLmv/AaYf+r2OP5i47fO4rWAgLvvdbR2zt9M6X1+XFTDcIdbEPSFREBoNoTEQFmcFvrB4CIm0yxpupXN7wOWxg2SE9fvxhJ98F7Hy1t9rBSm3xyp7cKTV+0upABu2BiAibuBj4AqgDNgC3GiM2eOVZjXwFawAcB7woDHmPHvbRUAr8OshAkCrMeaHvmbWETWAAb1d8Pq91lARg0Ukw4zzYd6nYe5K64I1Bq1dvfx602Ee31hKQ3sPi7Ji+YcLclhdkIqHPqv20NUM7fXWq7UGWqqsV1utfSHtsGoapt+62GGsi6M7yLrowcn1/b3WN/X+XutiGRzpdeEMsy6uMvDt3XjtN2ioCxH7HPaFdeACb4wVePq6T+bJu4Zg+qw0AzUcd8jJtL2d0NViBcnOJuhosMrc0WDtNy4EIlOsWlpEoh2gXHa+7Tz091t5CwqF4AgIjbWCUVictU94gvUeEm39/IIjrEDW3QZdrdaXg6YyaCqHyCRrIqKUfOtnO6C/zypjZ6OVp7A4K+iJnJrd/j5orYaORitQBgVbv6+weCvfI9XXY5Vx4MtBWPzwX1oG6++H9jr7Z5k0fPrebvv32mj9Pttqrd9xXI7V1BoWd/qxB/6+Y7IgYfbJ31HtPji00TrewM8+NMZ6sDM8wfrsCbe29XVbTbEtVVZZE/Os2rMxUPURHHwX6g9CwixIng8Jedb/syfMykvtfqgutl4rvmSlG4VRNwGJyPnA/caYq+zl+wCMMd/3SvNzYIMx5hl7eT9wsTGm0l7OAV7RADAKXa32P/Ix6yKcvgTick//Jx0H7d29vLC1jCf/dpjSujbSYkL5hwtyWbs8i6hQz7ifb0oxxq6ttENP28kA09tlvXrarG29nVbA7Omw9nHZtRA4ecHrbLJ/p2XWxcj0n6zxBIVYAcnlPllD6W61LlydzcAYmmwHah8DAbGn/fTjiduq6QTZgbiv27r4D1UTdAVBZCqEx1nl7W6zjuny2OWwA0VwhLXc0WBdDNvrBx3HA/G51sUPY10sW2uscw/U/FxB1gXYFWSdq7X6ZM0vIhlSF1oX1rZaa1v7cTvIdJ38cnI24YnWe0/H0D+X4Eirdt145OS9tdEKjrT+Jjoa7PwnndIsO6SQaLj+Sau2PwpjaQLKAI55LZdhfcsfLk0GMNxP6i4RuQUoAr5mjGkYIuN3AHcAZGdn+5DdaSYk0vqGMpIbwqMUHhzEzefn8IXzZrDh4xoe33iI/3ptLw/9+QCfPy+b2z6RS2pMqN/zMSmJnKyhEKCb5gPf2NvqrAtGe531LbSr1QpA7hDrYhscCVGpVg0jOt2691O5Ayp3WsFkoEnsRM0i1jp++3HoOG4dcyCwudzWMaLSrG/J/b3W+u42aK2Clmprn4ELvSfcapYbCI49HXbzXqfV0y1ruVX78YSfvE/TXA51B6C+xAoGkcnWYIlBoXZw7LO+lff3Wp+DQq3yRaZa56oqhupdULPHuphGpULSvJM1qIFmvZAY6z0iCSISrJ/T8VJrMMbjB63gMtDEN3CciEQ4fggqtkPVTqs5duYlMOsS6/zdrXbNoulkDbmr2foy0N126s9PXFB/wCprdxvkXgQzL7bK29lkfds/XnqyObS/1wqKqQUQO8MvX/p8qQFcD1xljPkne/lmYLkx5iteaV4Fvm+Mec9efgf4V2PMVns5h9NrAClAHVao/U8gzRjzD2fLiyNrAAG2s6yRn28s5fVdlbhdwrWLMvinT+YyPy060FlTSvloLDWAMiDLazkTqBhFmlMYY6q9Mvc48IoPeVET7JzMWB7+/FKOHW/nl+8d4vdbjvHCtjKWZsdy4/JsrjknnbBg7W2j1FTkyx2cLUCeiOSKSDCwFlg/KM164BaxrACaBtr/z0RE0rwWPwMUjyDfaoJlxYdz/7X5bLrvUr559XyaOnr4xvM7Wf69P/EfLxezt7I50FlUSo2QT88B2L18foLVDfQJY8x/icidAMaYn9ndQH8KrMTqBnqbMabI3vcZ4GIgEagGvm2M+aWI/AZYjNUEdBj44nBBQ5uAJg9jDB8eOs6zW47x6q5Kunv7WZQZw7WLM7h6YZpz7xUoNQnpg2DKbxrbu3lxWznPby1jT2UzIrAsJ57VBamsLNBgoFSgaQBQE+JgbSuvfFTJKzsrOFDTCsDS7FgunZfMxXOTWZAWjcs1/r0ZlFJnpgFATbiSmlbeKK7kzd3V7CpvAiAxMoRPzErg/FkJnD8zgRkJ4YgfurcppU7SAKACqrali40f1/KXj2vZVFpPbYs1SU1KdAjLcxNYnhPH4qw45qRGEhKkvYqUGk8aANSkYYzhYG0bmw7W8eHhBj48VE91sxUQPG5hbmoUCzNiyE+PoSAjhrkpUdrVVKkx0ACgJi1jDGUNHewsa2JXeRPF5U0UVzTR2N4DgEtgRkIEc1OimJcWxbzUKOalRpMdH673E5TywZhGA1XKn0SErPhwsuLDufoc6/GQgaBQXN7EvqoW9le1sL+6hTf3VDHwnSXU42JWUiR5yZHkpUSRlxzJnJQosuLDcWtgUGpYGgDUpOQdFFYtPPnMYEd3Hx9XWwHh4+oWPq5p5YNDx3lpx8kHz4ODXMxMjGBWciSzk6ygMDc1kpyECILcOgmeUgM0AKgpJSzYzaKsWBZlxZ6yvqWzh5KaVg5Ut3KgpoXS2jaKy5t4bVfliRqDxy1kx4eTmxjJrOQICtJjOCczhux47YmknEkDgJoWokI9LMmOY0l23CnrO3v6KKlptWoL1a0crmvjUF0bGz+upbvPGuI4KiSIzPhwMmJDyYgNY0F6NAszYslLicSjNQY1jWkAUNNaqMdNQYbVm8hbd28/H1e3UFzexJ7KZsobOihr6GDTwXraNlkTvwQHuchNiGBmUgS5iRHMTY1ibmoUMxMjCQ7SwKCmPg0AypGCg1xDBob+fsPh+jZ2lTexu6KZ0tpW9le18Paeanr7rbYkj1uYnRzFgrRo8tOjmZ8WzdzUKOIjdJpHNbVoN1ClfNDd28+hujb2VTWzt7KFPZXN7Klooq715ExTiZEh5KdHsygzhkVZsRRkxJAcFaL3F1TAaTdQpcYgOMh1oglozeKT62uaO9ln90jaV2U1Kf303VrsygKx4R7mpkQxPy2ahRkxLMyMYVZSpHZTVZOC1gCUGmft3b0Ulzezt7LZfobBqjV09Fj3FsI8bvtGcwwLM2JYkh1LbmKE1hSU3+iTwEoFUF+/obS2lV3l1tPOu8qsewwDQSEmzMM5mdbwF/np0RRkxJCjA+WpcaIBQKlJpq/fUFLTyo5jDWw/2shHZU0cqG45cbN5ICgszoplSXYsi7Pi9EazGhUNAEpNAV291nMLxeVN7DjWxEfHGtlf3UKfHRRmJISzOCuWxfbDcAvSogn16EB56uw0ACg1RbV397KrrIkdxxrZdrSBj441UdXcCYDbJcxKsp5qLsyJZ8XMeL2foE6jvYCUmqLCg4M4b2YC581MOLGuqqmTj8oa2V3eRHFFMxsP1PHi9nIAkqNCWJgRw+yUSOYkR3HujDideEcNSQOAUlNQakwoqTGpXJWfClijpx6qa2NTaT0flB5nf1ULGw/U0tNn1fCz4sO4cHYSS7NjyUuJYnZyJJEh+u/vdD41AYnISuBBwA38whjzwKDtYm9fDbQDf2+M2WZvewK4BqgxxhR47RMP/B7IAQ4DNxhjGs6WD20CUsp3PX39HK5rY3NpPRsP1LHpYD2tXb0nts9OjuTC2YlcNCeRwpx4okM9Acyt8qdR3wMQETfwMXAFUAZsAW40xuzxSrMa+ApWADgPeNAYc5697SKgFfj1oADwA+C4MeYBEVkHxBlj7j1bXjQAKDV6vX39HGvosAbGq2rhw8PH+fDQcbp6rUHxsuPDWZAWzcLMGJZmx7E4K1ZnYpsmxnIPYDlQYowptQ/0LLAG2OOVZg3WBd4Am0UkVkTSjDGVxpiNIpIzxHHXABfbn58CNgBnDQBKqdELcrvITbQGthtoOurs6aPocAMflTWyp6KZPZXNvLG7ykrvEuanRZ/S62hmYoTOwjaN+BIAMoBjXstlWN/yh0uTAVSe5bgpxphKAGNMpYgkD5VIRO4A7gDIzs72IbtKKV+FetxcmJfIhXmJJ9Y1tfew7WgDRUeOs/1oI3/YXs5vNh8BIDIkiIKMaBZlxlKYE8+ynDhiw/XZhKnKlwAwVLgf3G7kS5pRMcY8BjwGVhPQeBxTKXVmMeEeLpmXzCXzrO9kff2Gg7Wt7DjaeOJJ5if/dpifbywFIC85kgXp0cxLjWZ+WhRLsuKICdf7CVOBLwGgDMjyWs4EKkaRZrDqgWYiEUkDanzIi1JqgrldwpyUKOakRHHDMuvfvLOnj51lTXx4qJ5tRxspOtzAy17TcuYlR1KYE8eizFgWZsYwJyVKJ9eZhHwJAFuAPBHJBcqBtcDnB6VZD9xl3x84D2gaaN45i/XArcAD9vvLI8m4UipwQj1ulufGszw3/sS6po4edlc0se1IA1sON/DKzkqe+dBqGQ4OcjE3xZpDYX5aFAszY8lP16eYA83XbqCrgZ9gdQN9whjzXyJyJ4Ax5md2N9CfAiuxuoHeZowpsvd9ButmbyJQDXzbGPNLEUkAngOygaPA9caY42fLh/YCUmrq6O83HDnebg9+13hiHoXjbdYcCkEuYV5aFAszYk+MjJqXEqlBwQ90KAilVMAZY6hu7mJnWSMflTWy41gju8qaaO60nk9wu4SchPAT9xPyM6zRUZOjQgOc86lNh4JQSgWciJx4ivlKr6eYjx3vYFd5E/uqmtlf1cKu8iZe3XWyFTkpKsRuPrICw7zUaHITI3Ru5jHSAKCUCigRITshnOyEcK4+J+3E+ubOHvZWNFNc0cyeCmuCnfcPlp4Y3sLjFmYlRTIvNerEvMyzkyNJjwnTZxV8pAFAKTUpRYd6ThsEr7u3n9K6VvZXWVNw7qts5oNDx3nJqwdSmMfNrOQIZiVFMispktnJVpCYkRChU3EOogFAKTVlBAe5mJdqPXOwxmt9Y3s3+6taKKltpaTGeg3umhrmcTMnNYrchHBmJESQkxhOdnw42fERJEYGO3K0VA0ASqkpLzY8+LTaAlhzKZTUtLKvqoW9lc18XN3ClsMNvPxRBd79X8KD3cxKiiQvJZK85ChyEyOYkRDOjIRwwoOn72Vy+pZMKeV44cFBnJMZyzmZsaes7+zpo6yhnaPH2zla387h+nYO1rbyfkk9L24rPyVtanQoM5OsJqUZdu0hOz6crPiwKR8cpnbulVJqFEI9bmYnRzE7Oeq0bc2dPXZQaONwXRuldW2U1rbx0o5yWjp7T0kbG+4hIzaMzLgwZiTYtYZ46z09NmzS33PQAKCUUl6iQz0UZMRQkBFzynpjDI3tPRw53s6R+jbKGzsob+igvLGDkppW3t1fS7c9tDZYvZQyYsNIjg4lNTqUtJhQMuPCyIwPJysujPTYwNcgNAAopZQPRIS4iGDiIoJZnBV72vb+fkNVcydH6q0Acai+jYrGTqqbrek739jdeUqAAIgL95AWE0Z6bChpMWHWMxLRoaREh5ISHUJKTChRIUF+u0GtAUAppcaByyWkx1rf7M+flXDa9v5+Q21rF8eOt1PW0EFFUwcVjR1UNHZS1tDBlsMNNHX0nLZfRLCblJhQvveZhayYefpxx0IDgFJKTQCXS+xv9qEU5gydpr27l5rmLqqarZpDdXMnlU3We6wfhtjWAKCUUpNEeHAQOYlB5CRGTMj5dCANpZRyKA0ASinlUBoAlFLKoTQAKKWUQ2kAUEoph9IAoJRSDqUBQCmlHEoDgFJKOdSUmhReRGqBI6PcPRGoG8fsTBVOLLcTywzOLLcTywwjL/cMY0zS4JVTKgCMhYgUGWMKA52PiebEcjuxzODMcjuxzDB+5dYmIKWUcigNAEop5VBOCgCPBToDAeLEcjuxzODMcjuxzDBO5XbMPQCllFKnclINQCmllBcNAEop5VCOCAAislJE9otIiYisC3R+/EFEskTkXRHZKyK7ReRue328iLwtIgfs97hA53W8iYhbRLaLyCv2shPKHCsiz4vIPvt3fv50L7eI3GP/bReLyDMiEjodyywiT4hIjYgUe607YzlF5D772rZfRK4aybmmfQAQETfwMLAKWADcKCILApsrv+gFvmaMmQ+sAL5sl3Md8I4xJg94x16ebu4G9notO6HMDwJvGGPmAYuwyj9tyy0iGcA/A4XGmALADaxlepb5V8DKQeuGLKf9P74WyLf3ecS+5vlk2gcAYDlQYowpNcZ0A88CawKcp3FnjKk0xmyzP7dgXRAysMr6lJ3sKeC6gGTQT0QkE7ga+IXX6ule5mjgIuCXAMaYbmNMI9O83FhT2IaJSBAQDlQwDctsjNkIHB+0+kzlXAM8a4zpMsYcAkqwrnk+cUIAyACOeS2X2eumLRHJAZYAHwApxphKsIIEkBzArPnDT4B/Bfq91k33Ms8EaoEn7aavX4hIBNO43MaYcuCHwFGgEmgyxrzFNC7zIGcq55iub04IADLEumnb91VEIoEXgK8aY5oDnR9/EpFrgBpjzNZA52WCBQFLgUeNMUuANqZH08cZ2W3ea4BcIB2IEJGbApurSWFM1zcnBIAyIMtrOROr6jjtiIgH6+L/tDHmRXt1tYik2dvTgJpA5c8PLgCuFZHDWE17l4rIb5neZQbrb7rMGPOBvfw8VkCYzuW+HDhkjKk1xvQALwKfYHqX2duZyjmm65sTAsAWIE9EckUkGOuGyfoA52nciYhgtQnvNcb82GvTeuBW+/OtwMsTnTd/McbcZ4zJNMbkYP1e/2yMuYlpXGYAY0wVcExE5tqrLgP2ML3LfRRYISLh9t/6ZVj3uaZzmb2dqZzrgbUiEiIiuUAe8KHPRzXGTPsXsBr4GDgI/Hug8+OnMl6IVfXbCeywX6uBBKxeAwfs9/hA59VP5b8YeMX+PO3LDCwGiuzf90tA3HQvN/AdYB9QDPwGCJmOZQaewbrP0YP1Df8fz1ZO4N/ta9t+YNVIzqVDQSillEM5oQlIKaXUEDQAKKWUQ2kAUEoph9IAoJRSDqUBQCmlHEoDgFJKOZQGAKWUcqj/By0wCd/R6F81AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['train_loss'],label='train')\n",
    "plt.plot(history['val_loss'],label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.6568991523310548\n",
      "rmse: 0.13299525\n",
      "mape: 0.38943928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from numpy import sqrt\n",
    "\n",
    "def mape(a, b): \n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/a)[mask].mean()\n",
    "\n",
    "y_pred = model(torch.FloatTensor(merged_df[x_col].values))\n",
    "y_real = torch.FloatTensor(merged_df[y_col].values)\n",
    "print('r2:',r2_score(y_real.detach().numpy(),y_pred.detach().numpy()))\n",
    "print('rmse:',sqrt(mean_squared_error(y_real.detach().numpy(),y_pred.detach().numpy())))\n",
    "print('mape:',mape(y_real.detach().numpy(),y_pred.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "for v,col in zip(y_pred,y_col):\n",
    "    df_pred[col] = v.detach().numpy()\n",
    "df_real = pd.DataFrame()\n",
    "for v,col in zip(y_real,y_col):\n",
    "    df_real[col] = v.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2_score</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.22</th>\n",
       "      <td>0.994646</td>\n",
       "      <td>0.026991</td>\n",
       "      <td>0.055353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.23</th>\n",
       "      <td>0.752868</td>\n",
       "      <td>0.139220</td>\n",
       "      <td>0.485589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.15</th>\n",
       "      <td>0.913852</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.220196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.2</th>\n",
       "      <td>0.881055</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.478650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.11</th>\n",
       "      <td>0.825993</td>\n",
       "      <td>0.106881</td>\n",
       "      <td>0.398968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.1</th>\n",
       "      <td>0.903884</td>\n",
       "      <td>0.073663</td>\n",
       "      <td>0.448968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.7</th>\n",
       "      <td>0.857233</td>\n",
       "      <td>0.085308</td>\n",
       "      <td>0.266486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.21</th>\n",
       "      <td>-0.331745</td>\n",
       "      <td>0.309646</td>\n",
       "      <td>2.542251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.8</th>\n",
       "      <td>0.874890</td>\n",
       "      <td>0.092661</td>\n",
       "      <td>0.243605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.4</th>\n",
       "      <td>0.923896</td>\n",
       "      <td>0.085830</td>\n",
       "      <td>0.308224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.22</th>\n",
       "      <td>0.877508</td>\n",
       "      <td>0.096844</td>\n",
       "      <td>0.249228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.3</th>\n",
       "      <td>0.698020</td>\n",
       "      <td>0.170608</td>\n",
       "      <td>0.183899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.4</th>\n",
       "      <td>0.704291</td>\n",
       "      <td>0.130631</td>\n",
       "      <td>0.427507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2</th>\n",
       "      <td>0.717661</td>\n",
       "      <td>0.132827</td>\n",
       "      <td>0.250832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.17</th>\n",
       "      <td>0.767351</td>\n",
       "      <td>0.105961</td>\n",
       "      <td>0.177604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.16</th>\n",
       "      <td>0.810621</td>\n",
       "      <td>0.099452</td>\n",
       "      <td>0.294669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.13</th>\n",
       "      <td>0.858206</td>\n",
       "      <td>0.099845</td>\n",
       "      <td>0.295584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.2</th>\n",
       "      <td>0.804122</td>\n",
       "      <td>0.115810</td>\n",
       "      <td>0.328086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.9</th>\n",
       "      <td>0.648769</td>\n",
       "      <td>0.150818</td>\n",
       "      <td>0.276046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.12</th>\n",
       "      <td>0.896481</td>\n",
       "      <td>0.077585</td>\n",
       "      <td>0.335101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.24</th>\n",
       "      <td>0.794383</td>\n",
       "      <td>0.125144</td>\n",
       "      <td>0.449304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.18</th>\n",
       "      <td>0.830394</td>\n",
       "      <td>0.101652</td>\n",
       "      <td>0.153428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.15</th>\n",
       "      <td>0.710926</td>\n",
       "      <td>0.126276</td>\n",
       "      <td>0.288823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.12</th>\n",
       "      <td>0.641379</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.498372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.6</th>\n",
       "      <td>0.346277</td>\n",
       "      <td>0.181694</td>\n",
       "      <td>0.309267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.7</th>\n",
       "      <td>0.721071</td>\n",
       "      <td>0.120602</td>\n",
       "      <td>0.191669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.18</th>\n",
       "      <td>0.462620</td>\n",
       "      <td>0.204126</td>\n",
       "      <td>0.325919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.16</th>\n",
       "      <td>0.880786</td>\n",
       "      <td>0.088040</td>\n",
       "      <td>0.197040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.5</th>\n",
       "      <td>0.552665</td>\n",
       "      <td>0.176034</td>\n",
       "      <td>0.835874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.9</th>\n",
       "      <td>0.950934</td>\n",
       "      <td>0.071718</td>\n",
       "      <td>0.112753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.20</th>\n",
       "      <td>0.846868</td>\n",
       "      <td>0.116384</td>\n",
       "      <td>0.734928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSARRDS2.8</th>\n",
       "      <td>0.620802</td>\n",
       "      <td>0.177959</td>\n",
       "      <td>0.462232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductDSLGORDS2.3</th>\n",
       "      <td>0.827280</td>\n",
       "      <td>0.161063</td>\n",
       "      <td>0.289260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     r2_score      rmse      mape\n",
       "ProductDSARRDS2.22   0.994646  0.026991  0.055353\n",
       "ProductDSLGORDS2.23  0.752868  0.139220  0.485589\n",
       "ProductDSARRDS2.15   0.913852  0.078946  0.220196\n",
       "ProductDSARRDS2.2    0.881055  0.099057  0.478650\n",
       "ProductDSARRDS2.11   0.825993  0.106881  0.398968\n",
       "ProductDSARRDS2.1    0.903884  0.073663  0.448968\n",
       "ProductDSARRDS2.7    0.857233  0.085308  0.266486\n",
       "ProductDSARRDS2.21  -0.331745  0.309646  2.542251\n",
       "ProductDSLGORDS2.8   0.874890  0.092661  0.243605\n",
       "ProductDSARRDS2.4    0.923896  0.085830  0.308224\n",
       "ProductDSLGORDS2.22  0.877508  0.096844  0.249228\n",
       "ProductDSARRDS2.3    0.698020  0.170608  0.183899\n",
       "ProductDSLGORDS2.4   0.704291  0.130631  0.427507\n",
       "ProductDSARRDS2      0.717661  0.132827  0.250832\n",
       "ProductDSARRDS2.17   0.767351  0.105961  0.177604\n",
       "ProductDSARRDS2.16   0.810621  0.099452  0.294669\n",
       "ProductDSARRDS2.13   0.858206  0.099845  0.295584\n",
       "ProductDSLGORDS2.2   0.804122  0.115810  0.328086\n",
       "ProductDSLGORDS2.9   0.648769  0.150818  0.276046\n",
       "ProductDSLGORDS2.12  0.896481  0.077585  0.335101\n",
       "ProductDSLGORDS2.24  0.794383  0.125144  0.449304\n",
       "ProductDSLGORDS2.18  0.830394  0.101652  0.153428\n",
       "ProductDSLGORDS2.15  0.710926  0.126276  0.288823\n",
       "ProductDSARRDS2.12   0.641379  0.137540  0.498372\n",
       "ProductDSARRDS2.6    0.346277  0.181694  0.309267\n",
       "ProductDSLGORDS2.7   0.721071  0.120602  0.191669\n",
       "ProductDSARRDS2.18   0.462620  0.204126  0.325919\n",
       "ProductDSLGORDS2.16  0.880786  0.088040  0.197040\n",
       "ProductDSARRDS2.5    0.552665  0.176034  0.835874\n",
       "ProductDSARRDS2.9    0.950934  0.071718  0.112753\n",
       "ProductDSLGORDS2.20  0.846868  0.116384  0.734928\n",
       "ProductDSARRDS2.8    0.620802  0.177959  0.462232\n",
       "ProductDSLGORDS2.3   0.827280  0.161063  0.289260"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics = pd.DataFrame()\n",
    "for a,b,c in zip(y_real,y_pred,y_col):\n",
    "    df_metrics.loc[c,'r2_score'] = r2_score(a.detach().numpy(),b.detach().numpy())\n",
    "    df_metrics.loc[c,'rmse'] = sqrt(mean_squared_error(a.detach().numpy(),b.detach().numpy()))\n",
    "    df_metrics.loc[c,'mape'] = mape(a.detach().numpy(),b.detach().numpy())\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "709221cd6e8e8bb1271bebb288222f3d75e028518a70d134b83bcc07aeaab82d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
